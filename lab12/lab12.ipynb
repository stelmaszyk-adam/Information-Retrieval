{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOME IMPORTS\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "import multiprocessing\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET SOME ENVIRONMENTAL VARIABLES\n",
    "os.environ['PYSPARK_PYTHON']=\"python3.6\"\n",
    "os.environ['SPARK_LOCAL_HOSTNAME']=\"localhost\"\n",
    "os.environ['SPARK_HOME']=\"/home/i/Downloads/spark-2.2.1-bin-hadoop2.7\"\n",
    "os.environ['JAVA_HOME']=\"/usr/lib/jvm/java-1.8.0-openjdk-amd64/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK IF FINDSPARK WORKS CORRECTLY\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=InvertedIndex, master=local[1]) created by __init__ at <ipython-input-5-0f272c506c2e>:5 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-ff8119e997e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# START SPARK CONTEXT ON LOCAL MACHINE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m##------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# GO TO LOCALHOST:4040 and ....\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/spark-2.2.1-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m~/Downloads/spark-2.2.1-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    297\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 299\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=InvertedIndex, master=local[1]) created by __init__ at <ipython-input-5-0f272c506c2e>:5 "
     ]
    }
   ],
   "source": [
    "# START SPARK CONTEXT ON LOCAL MACHINE\n",
    "sc = SparkContext(\"local\", appName=\"Test\")\n",
    "##------------------------------------\n",
    "# GO TO LOCALHOST:4040 and ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STOP SPARK CONTEXT\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of logical CPUs is 4\n"
     ]
    }
   ],
   "source": [
    "# OBTAIN THE NUMBER OF LOGICAL CPUs\n",
    "cpus = multiprocessing.cpu_count()\n",
    "print(\"The number of logical CPUs is \" + str(cpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Compute the value of PI using Monte Carlo Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise is solved. Your task is to read and analyse the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this method generates one sample point and verifies whether it is inside a circle or not.\n",
    "# The input is passed via filter method, however, we do not need it here\n",
    "def inside(inValue):\n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method estimates the value of PI\n",
    "def computePI_MonteCarlo_v1(sc, samples, partitions):\n",
    "    # Create Resilient Distributed Dataset (RDDs) containing SAMPLES elements.\n",
    "    # This data is distributed (parallelized) among available nodes (here, CPUs - partitions).\n",
    "    dff = sc.parallelize(range(0, samples), partitions)\n",
    "    # Filter out these samples that are not inside a circle.\n",
    "    # For this purpose, Inside method is run and returns\n",
    "    # true/false (for each data element) with appropriate probability distribution\n",
    "    # Why do we generate samples \"on fly\"?\n",
    "    #\n",
    "    # bo procesor generuje liczby pseudolosowo bazując na dacie - NIE\n",
    "    # bo to bardziej pozwala zrównoleglić procesy\n",
    "    #\n",
    "    filtered = dff.filter(inside)\n",
    "    # count the number of hits\n",
    "    left = filtered.count()\n",
    "    # Estimate the value of PI and return it\n",
    "    return 4.0 * float(left) / float(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo simulation for 10000000 samples\n",
      "True value of PI = 3.1415926535...\n",
      "  Number of CPUs = 1 | Time = 5.2769 s | Result(PI) = 3.14155240\n",
      "  Number of CPUs = 2 | Time = 2.6014 s | Result(PI) = 3.14170960\n",
      "  Number of CPUs = 3 | Time = 2.7001 s | Result(PI) = 3.14127160\n",
      "  Number of CPUs = 4 | Time = 2.7269 s | Result(PI) = 3.14212160\n"
     ]
    }
   ],
   "source": [
    "### ESTIMATE VALUE OF PI \n",
    "samples = 10000000\n",
    "\n",
    "print(\"Monte Carlo simulation for \" + str(samples) + \" samples\")\n",
    "print(\"True value of PI = 3.1415926535...\")\n",
    "\n",
    "## i = number of nodes (CPUs)\n",
    "for i in range(1, cpus + 1):\n",
    "    master = \"local[\"+str(i)+\"]\" \n",
    "    sc = SparkContext(master, appName=\"PI_MonteCarlo\")\n",
    "    start_time = time.time()\n",
    "    piValue = computePI_MonteCarlo_v1(sc, samples, i)\n",
    "    elapsed = time.time() - start_time\n",
    "    print(\"  Number of CPUs = %i | Time = %.4f s | Result(PI) = %.8f\" % (i, elapsed, piValue))  \n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercise 2: Wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy collection 1: 3 short documents\n",
    "# create RDD divided into n-paritions\n",
    "def getSmallCollection_EX1(sc, partitions):\n",
    "    doc1 = \"Roses,are red \"\n",
    "    doc2 = \"Roses are roses\"\n",
    "    doc3 = \"The Sun is red.\"\n",
    "    rdd1 = sc.parallelize([doc1, doc2, doc3], partitions)\n",
    "    return rdd1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Dummy collection 2: ~200 documents about animals (ant.html, dog.html, panda.html, hedgehog.html, etc.). For this purpose, download www.cs.put.poznan.pl/mtomczyk/ir/lab6/pages.zip, unzip, and copy \"pages\" folder into your working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLargeCollection_EX1(sc, partitions):\n",
    "    DOCS = sc.wholeTextFiles(\"./pages/\", partitions)\n",
    "    rdd1 = DOCS.map(lambda x: x[1])\n",
    "    return rdd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a given text \"x\", this method performs simple tokenization and normalization (returns a list of terms)\n",
    "def tokenizeAndNormalize(x):\n",
    "    return [s.lower() for s in re.split(' |;|,|\\t|\\n|\\.', x) if len(s) > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Init spark context (1 core):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local[1]\", appName=\"Word_count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) TODO: Collect the data (getSmallCollection_EX1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Roses,are red ', 'Roses are roses', 'The Sun is red.']\n"
     ]
    }
   ],
   "source": [
    "rdd1 = getSmallCollection_EX1(sc, 1)\n",
    "# if you whish to print data stored in rdd, use print(rdd.collect())\n",
    "print(rdd1.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) TODO: Firslty, you should tokenize all documents. For this purpose use flatMap function (rdd2 = rdd1.flatMap) where you pass tokenizeAndNormalize method. There are two methods: map and flatMap. Both produce an output for each element of RDD object. The difference is that map keeps produced elements organised and flatMap puts them into a single list, e.g.: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 2), ('b', 3)]\n",
      "['a', 2, 'b', 3]\n"
     ]
    }
   ],
   "source": [
    "tempRDD = sc.parallelize([(\"a\", 1), (\"b\", 2)])\n",
    "print(tempRDD.map(lambda x: (x[0], x[1]+1)).collect())\n",
    "print(tempRDD.flatMap(lambda x: (x[0], x[1]+1)).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['roses', 'are', 'red', 'roses', 'are', 'roses', 'the', 'sun', 'is', 'red']\n"
     ]
    }
   ],
   "source": [
    "# Complete the task here (flatMap with tokenizeAndNormalize):\n",
    "rdd2 = rdd1.flatMap(lambda x: tokenizeAndNormalize(x))\n",
    "print(rdd2.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) TODO: Now for each term produce (term, 1). Use map (why not flatMap?) with lambda function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('roses', 1), ('are', 1), ('red', 1), ('roses', 1), ('are', 1), ('roses', 1), ('the', 1), ('sun', 1), ('is', 1), ('red', 1)]\n"
     ]
    }
   ],
   "source": [
    "rdd3 = rdd2.map(lambda x: ( x ,1))\n",
    "print(rdd3.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) TODO: Now it is time to group the results. Use groupByKey method. When any \"...byKey\" method is invoked, the first element of a stored object is treated as a key. When invoking this method, you should also invoke .mapValues(list) so that all corresponding values will be stored in a single list. E.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', [1, 1])]\n"
     ]
    }
   ],
   "source": [
    "tempRDD = sc.parallelize([(\"a\", 1), (\"a\", 1)])\n",
    "print(tempRDD.groupByKey().mapValues(list).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('roses', [1, 1, 1]), ('are', [1, 1]), ('red', [1, 1]), ('the', [1]), ('sun', [1]), ('is', [1])]\n"
     ]
    }
   ],
   "source": [
    "# Complete the task here:\n",
    "rdd4 = rdd3.groupByKey().mapValues(list)\n",
    "print(rdd4.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) TODO: Now you could use countByKey method but it returns a dictionarty. Use map function again to sum the elements of a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('roses', 3), ('are', 2), ('red', 2), ('the', 1), ('sun', 1), ('is', 1)]\n"
     ]
    }
   ],
   "source": [
    "rdd5 = rdd4.map(lambda x: (x[0], sum(x[1])))\n",
    "print(rdd5.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) TODO: It is almost done but we wish the objects to be sorted (alphabetically). You can use sortByKey method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('are', 2), ('is', 1), ('red', 2), ('roses', 3), ('sun', 1), ('the', 1)]\n"
     ]
    }
   ],
   "source": [
    "rdd6 = rdd5.sortByKey()\n",
    "print(rdd6.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) TODO: Done. Bout it could be done in another way. Instead of grouping by key (rdd4) and counting the number of \"1\"s (rdd5), you could use reduceByKey method. reduceByKey \"merges\" all object with the same key. Similar to groupByKey, however, instead of grouping, a new value is computed by provided function, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 4), ('b', 2)]\n"
     ]
    }
   ],
   "source": [
    "tempRDD = sc.parallelize([(\"a\", 1), (\"b\", 2), (\"a\", 3)])\n",
    "print(tempRDD.reduceByKey(lambda x, y: x + y).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('roses', 3), ('are', 2), ('red', 2), ('the', 1), ('sun', 1), ('is', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Complete the task here. Use rdd3 object to compute rdd7.\n",
    "rdd7 = rdd3.reduceByKey(lambda x, y: x + y)\n",
    "print(rdd7.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10) TODO: Sort the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('are', 2), ('is', 1), ('red', 2), ('roses', 3), ('sun', 1), ('the', 1)]\n"
     ]
    }
   ],
   "source": [
    "rdd8 = rdd7.sortByKey()\n",
    "print(rdd8.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11) TODO: Complete the method doWordCount (just copy your code, use groupByKey + map(sum) version; should return last rdd object):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doWordCount(sc, collection, partitions):\n",
    "    rdd1 = collection\n",
    "    rdd2 = rdd1.flatMap(lambda x: tokenizeAndNormalize(x))\n",
    "    rdd3 = rdd2.map(lambda x: ( x ,1))\n",
    "    rdd4 = rdd3.reduceByKey(lambda x, y: x + y)\n",
    "    rdd5 = rdd4.sortByKey()\n",
    "    return rdd5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12) TODO: Run the script and observe the results (why is the best time for 1CPU?):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00348663330078125\n",
      "Number of CPUs = 1 | Time = 0.0227 s \n",
      "0.0026862621307373047\n",
      "Number of CPUs = 2 | Time = 0.7214 s \n",
      "0.0026967525482177734\n",
      "Number of CPUs = 3 | Time = 0.6691 s \n",
      "0.0019447803497314453\n",
      "Number of CPUs = 4 | Time = 0.7175 s \n"
     ]
    }
   ],
   "source": [
    "## i = number of nodes (CPUs). \n",
    "for i in range(1, cpus + 1):\n",
    "    master = \"local[\"+str(i)+\"]\" \n",
    "    sc = SparkContext(master, appName=\"WordCount\")\n",
    "    start_time = time.time()\n",
    "    rdd1 = getSmallCollection_EX1(sc, i)\n",
    "    elapsed = time.time() - start_time\n",
    "    print(elapsed)\n",
    "    \n",
    "    computedData = doWordCount(sc, rdd1, i)\n",
    "    elapsed = time.time() - start_time\n",
    "    print(\"Number of CPUs = %i | Time = %.4f s \" % (i, elapsed))  \n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13) TODO: Modyfy the above script (work on a copy, use the cell below) so that the top 3 most common words are printed. Use 1-2CPUs. computedData is an RDD object so you can use sortBy function to resort the elements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs = 1 | Time = 0.0169 s \n",
      "   0 : 'roses' occured 3 times\n",
      "   1 : 'are' occured 2 times\n",
      "   2 : 'red' occured 2 times\n",
      "Number of CPUs = 2 | Time = 0.8432 s \n",
      "   0 : 'roses' occured 3 times\n",
      "   1 : 'are' occured 2 times\n",
      "   2 : 'red' occured 2 times\n"
     ]
    }
   ],
   "source": [
    "# do the task here\n",
    "for i in [1,2]:\n",
    "    master = \"local[\"+str(i)+\"]\" \n",
    "    sc = SparkContext(master, appName=\"WordCount\")\n",
    "    start_time = time.time()\n",
    "    rdd1 = getSmallCollection_EX1(sc, i)\n",
    "    computedData = doWordCount(sc, rdd1, i)\n",
    "    rddSort = computedData.sortBy(lambda x: -x[1])\n",
    "    elapsed = time.time() - start_time\n",
    "    print(\"Number of CPUs = %i | Time = %.4f s \" % (i, elapsed))  \n",
    "    ### PRINT HERE \n",
    "    sortedData = rddSort.collect()\n",
    "    for i in range(0, 3): #print top 3\n",
    "        print(\"   %i : '%s' occured %d times\" % (i, sortedData[i][0], sortedData[i][1]))\n",
    "    ###\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14) TODO: Repeat the experiment for 1-2CPUs and for 2nd collection (much larger). Compare computation times and print the top 20 most common words. Are the results (the most frequent words) similar to the list of english stop words? Why is the difference in time not as big as in \"PI\" example?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs = 1 | Time = 1.2391 s \n",
      "   0 : 'the' occured 3027 times\n",
      "   1 : 'and' occured 1910 times\n",
      "   2 : 'of' occured 1553 times\n",
      "   3 : 'in' occured 1165 times\n",
      "   4 : 'are' occured 1031 times\n",
      "   5 : 'to' occured 962 times\n",
      "   6 : 'a' occured 769 times\n",
      "   7 : 'is' occured 622 times\n",
      "   8 : 'as' occured 560 times\n",
      "   9 : 'species' occured 558 times\n",
      "   10 : 'they' occured 370 times\n",
      "   11 : 'for' occured 362 times\n",
      "   12 : 'with' occured 352 times\n",
      "   13 : 'have' occured 344 times\n",
      "   14 : 'their' occured 326 times\n",
      "   15 : 'or' occured 306 times\n",
      "   16 : 'from' occured 269 times\n",
      "   17 : 'by' occured 244 times\n",
      "   18 : 'on' occured 230 times\n",
      "   19 : 'which' occured 214 times\n",
      "Number of CPUs = 2 | Time = 3.6531 s \n",
      "   0 : 'the' occured 3027 times\n",
      "   1 : 'and' occured 1910 times\n",
      "   2 : 'of' occured 1553 times\n",
      "   3 : 'in' occured 1165 times\n",
      "   4 : 'are' occured 1031 times\n",
      "   5 : 'to' occured 962 times\n",
      "   6 : 'a' occured 769 times\n",
      "   7 : 'is' occured 622 times\n",
      "   8 : 'as' occured 560 times\n",
      "   9 : 'species' occured 558 times\n",
      "   10 : 'they' occured 370 times\n",
      "   11 : 'for' occured 362 times\n",
      "   12 : 'with' occured 352 times\n",
      "   13 : 'have' occured 344 times\n",
      "   14 : 'their' occured 326 times\n",
      "   15 : 'or' occured 306 times\n",
      "   16 : 'from' occured 269 times\n",
      "   17 : 'by' occured 244 times\n",
      "   18 : 'on' occured 230 times\n",
      "   19 : 'which' occured 214 times\n"
     ]
    }
   ],
   "source": [
    "# do the task here\n",
    "for i in [1,2]:\n",
    "    master = \"local[\"+str(i)+\"]\" \n",
    "    sc = SparkContext(master, appName=\"WordCount\")\n",
    "    start_time = time.time()\n",
    "    rdd1 = getLargeCollection_EX1(sc, i)\n",
    "    computedData = doWordCount(sc, rdd1, i)\n",
    "    rddSort = computedData.sortBy(lambda x: -x[1])\n",
    "    elapsed = time.time() - start_time\n",
    "    print(\"Number of CPUs = %i | Time = %.4f s \" % (i, elapsed))  \n",
    "    ### PRINT HERE\n",
    "    sortedData = rddSort.collect()\n",
    "    for i in range(0, 20): #print top 20\n",
    "        print(\"   %i : '%s' occured %d times\" % (i, sortedData[i][0], sortedData[i][1]))\n",
    "    ###\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Inverted Index + Word Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you are asked to construct inverted index in the following form: (term, the number of doccuments in which the term occurs , sorted list of docIDs]. For instance: [...,(\"roses\", 2, [0, 1]),...] -> term \"roses\" occurs in two documents: termIDs = 0 and 1. The \"get...Collection\" methods are slightly modified. Both return: rdd object, list of the names of the documents, and a dictionary (docID -> document name):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSmallCollection_EX2(sc, partitions):\n",
    "    doc1 = \"Roses,are red \"\n",
    "    doc2 = \"Roses are roses\"\n",
    "    doc3 = \"The Sun in red.\"\n",
    "    rdd1 = sc.parallelize([doc1, doc2, doc3], partitions)\n",
    "    docNames = [\"doc1\", \"doc2\", \"doc3\"]\n",
    "    docIDs = {0: docNames[0], 1: docNames[1], 2: docNames[2]}\n",
    "    return rdd1, docNames, docIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLargeCollection_EX2(sc, partitions):\n",
    "    DOCS = sc.wholeTextFiles(\"./pages/\", partitions)\n",
    "    rdd1 = DOCS.map(lambda x: x[1])\n",
    "    rdd2 = DOCS.map(lambda x: x[0])\n",
    "    docNames = rdd2.collect()\n",
    "    docIDs = [i for i in range(0, len(docNames))]\n",
    "    return rdd1, docNames, docIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whichOccurs(rdd):\n",
    "    x = rdd[0]\n",
    "    index = rdd[1]\n",
    "    return [(s.lower(), index) for s in re.split(' |;|,|\\t|\\n|\\.', x) if len(s) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeAndNormalize(x):\n",
    "    return [s.lower() for s in re.split(' |;|,|\\t|\\n|\\.', x) if len(s) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeAndNormalize(x):\n",
    "    return [s.lower() for s in re.split(' |;|,|\\t|\\n|\\.', x) if len(s) > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: do the task and verify the results using the small collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doInvertedIndex(sc, collection, partitions):\n",
    "    rddy___ = rddx.map(lambda x: (x[0], x[1][0], x[1][1]))\n",
    "    rddy = rddx.map(lambda x: (x[0], len(x[1][1]), x[1][1]))\n",
    "    rdd6 = rddy.sortBy(lambda x: -x[1] )\n",
    "    \n",
    "    return rdd6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12) Run the following script and verify the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rddx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-0f272c506c2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mrdd1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocNames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocIDs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetSmallCollection_EX2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mcomputedData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoInvertedIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrdd1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mrddSort\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomputedData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msortBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0melapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-536ff11087a0>\u001b[0m in \u001b[0;36mdoInvertedIndex\u001b[0;34m(sc, collection, partitions)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdoInvertedIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mrddy___\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrddx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mrddy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrddx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mrdd6\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrddy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msortBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rddx' is not defined"
     ]
    }
   ],
   "source": [
    "## i = number of nodes (CPUs). \n",
    "#Why the best time is for 1CPU???\n",
    "for i in [1,2]:\n",
    "    master = \"local[\"+str(i)+\"]\" \n",
    "    sc = SparkContext(master, appName=\"InvertedIndex\")\n",
    "    start_time = time.time()\n",
    "    rdd1, docNames, docIDs = getSmallCollection_EX2(sc, i)\n",
    "    computedData = doInvertedIndex(sc, rdd1, i)\n",
    "    rddSort = computedData.sortBy(lambda x: -x[1])\n",
    "    elapsed = time.time() - start_time\n",
    "    print(\"Number of CPUs = %i | Time = %.4f s \" % (i, elapsed))  \n",
    "    ### PRINT HERE \n",
    "    sortedData = rddSort.collect()\n",
    "    for i in range(0, 3): #print top 3\n",
    "        print(f\"   {i} : '{sortedData[i][0]}' occured in {sortedData[i][1]} documents: {sortedData[i][2]}\")\n",
    "    ###\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12) Run the following script and verify if it is faster for 2 cores. Lastly, compare the obtained results with the results of exercise 2 (word count). Are the rankings corellated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=InvertedIndex, master=local[1]) created by __init__ at <ipython-input-61-0f272c506c2e>:5 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-eb70caa5c423>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmaster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"local[\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"InvertedIndex\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mrdd1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocNames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocIDs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetLargeCollection_EX2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/spark-2.2.1-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m~/Downloads/spark-2.2.1-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    297\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 299\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=InvertedIndex, master=local[1]) created by __init__ at <ipython-input-61-0f272c506c2e>:5 "
     ]
    }
   ],
   "source": [
    "## i = number of nodes (CPUs). \n",
    "#Why the best time is for 1CPU???\n",
    "for i in [1,2]:\n",
    "    master = \"local[\"+str(i)+\"]\" \n",
    "    sc = SparkContext(master, appName=\"InvertedIndex\")\n",
    "    start_time = time.time()\n",
    "    rdd1, docNames, docIDs = getLargeCollection_EX2(sc, i)\n",
    "    computedData = doInvertedIndex(sc, rdd1, i)\n",
    "    rddSort = computedData.sortBy(lambda x: -x[1])\n",
    "    elapsed = time.time() - start_time\n",
    "    print(\"Number of CPUs = %i | Time = %.4f s \" % (i, elapsed))  \n",
    "    ### PRINT HERE \n",
    "    sortedData = rddSort.collect()\n",
    "    for i in range(0, 20): #print top 3\n",
    "        print(f\"   {i} : '{sortedData[i][0]}' occured in {sortedData[i][1]} documents: {sortedData[i][2][:3]}...\")\n",
    "    ###\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
